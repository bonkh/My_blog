<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Language identification models evaluation</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/style.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">
</head>

<body>
    <div id="imageModal" class="modal">
        <span class="close">&times;</span>
        <img class="modal-content" id="modalImg">
    </div>


    <aside class="sidebar" id="sidebar">
        <h2 class="sidebar-title">My Blog</h2>
        <nav class="sidebar-nav">
            <ul>
                <li><a href="../../../index.html">Home</a></li>
                <li><a href="../../../origami_index.html">Origami Blog</a></li>
                <li><a href="../../../data_science_index.html" class="active">Data Science</a></li>
            </ul>
        </nav>
        <div class="sidebar-footer">
            <h3 class="sidebar-subtitle">Contact Me</h3>
            <nav class="sidebar-nav">
                <ul class="contact-links">
                    <li>
                        <a href="https://github.com/bonkh" target="_blank">
                            <img src="../../../Images/git_icon.png" alt="GitHub" class="icon"> GitHub
                        </a>
                    </li>
                    <li>
                        <a href="https://www.kaggle.com/caokhoihuynh" target="_blank">
                            <img src="../../../Images/kaggle_icon.png" alt="Kaggle" class="icon"> Kaggle
                        </a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/khoi-huynh-b528b3205/" target="_blank">
                            <img src="../../../Images/linkedin_icon.png" alt="LinkedIn" class="icon"> LinkedIn
                        </a>
                    </li>
                    <li>
                        <a href="mailto:caokhoi20092003@gmail.com">
                            <img src="../../../Images/gmail_icon.png" alt="Email" class="icon"> Email
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
        <button id="collapse-button" class="collapse-button">Â«</button>
    </aside>

    <div class="page-container">
        <main class="main-content">
            <section class="project-detail">
                <div class="project-info">

                    <div class="info-column">
                        <h2>â€” PROJECT NAME</h2>
                        <p>Language identification models evaluation</p>

                        <h2>â€” TIME</h2>
                        <p>11/2023 - 01/2024</p>

                        <h2>â€” TEAM SIZE</h2>
                        <p>Hoan Nguyen (Superviser)</p>
                        <p>Huynh Cao Khoi </p>
                    </div>


                    <div class="info-column">
                        <h2>â€” ROLE</h2>
                        <ul>
                            <li>Data Collecting</li>
                            <li>Data Preprocessing</li>
                            <li>Exploratory Data Analysis (EDA)</li>
                            <li>Model Evaluation</li>
                            <li>Evaluation result analysis</li>
                        </ul>

                        <h2>â€” TOOLS</h2>
                        <ul>
                            <li>Data Collecting : Hugging Face API</li>
                            <li>Data Preprocessing: Polars, sentence_splitter, pyspark,unicodedata, semhash, cleanlab,
                                ...</li>
                            <li>EDA: Matplotlib, Seaborn,...</li>
                            <li>Model Evaluation: pytorch</li>
                            <li>Evaluation result analysis: </li>
                        </ul>
                    </div>

                </div>

                <div class="project-description">
                    <p>
                        This project is the work in my intership for Mr.Hoan Nguyen. In this project,I try to research
                        and find the current state of the language indentification problem,
                        which is an important element in many language models.
                    </p>

                    <p>
                        In this project, I have to do these bellow main tasks:
                    </p>

                    <ul>
                        <li>Find and study about text datasets for language identification task.</li>
                        <li>Preprocess and combined all datasets together</li>
                        <li>EDA the dataset</li>
                        <li>Evaluate models in <a href="https://batdongsan.com.vn/" target="_blank">this list </a> in
                            many metrics (even the runtime performance) with the combined dataset.</li>
                        <li>Analyze the results </li>
                    </ul>

                    <h3>Task 01: Collect text dataset</h3>

                    <p>
                        The full list of text dataset can be found in <a
                            href="https://docs.google.com/spreadsheets/d/1G12FaSMelNX87dclhm9dE3d5ZRO99B2hoFE1ufB2Zvg/edit?usp=sharing">this
                            spreedsheet</a>
                    </p>
                    <p>
                        These datasets consist of training text dataset, or benchmark dataset, which is published mostly
                        in recent years, cover a wide range of topics and domains
                    </p>

                    <p>
                        The collected data is in this format:
                    </p>

                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/raw_data_process.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Raw data</figcaption>
                        </figure>
                    </div>

                    <p>
                        As you can see, these dataset was consist of two main attributes:
                    </p>
                    <ul>
                        <li>Text: This is the main text </li>
                        <li>Language: The language code, which normally in the format language code(ISO 639-3) -
                            language script(ISO 15924)</li>

                    </ul>

                    <h3>
                        Task 02: Merge and preprocess the combined text datasets
                    </h3>

                    <p>
                        After downloading the dataset from multiple sources, I performed some simple preprocessing steps
                        in each dataset.
                    </p>

                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/raw_data_process.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Raw data processing pipeline</figcaption>
                        </figure>
                    </div>

                    <p>
                        In the handling long text step, I used <a
                            href="https://pypi.org/project/iges-sentence-splitter/">sentence-split</a> package to split
                        the text into sentences more accurately.
                    </p>

                    <p>
                        After that, I combined all the sources into one dataset, using this process:
                    </p>

                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/combined_data_process.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Processing pipeline for combined data</figcaption>
                        </figure>
                    </div>

                    <p>
                        With the combined dataset, I performed these main steps to clean it:
                    </p>
                    <h4>
                        Step 01: Clean the programming language pattern
                    </h4>

                    <p>
                        Programming language is not the natural language, I considered it as noise in a LID dataset
                    </p>
                    <p>
                        I simply used <a href="https://docs.python.org/3/library/re.htmlz">re</a> package to find the
                        programming language pattern inside the text.
                        The text containing mostly programming language pattern will be removed. In others, I cleand all
                        the possible patterns.
                    </p>
                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/programming_language_removal.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Progamming language removing pipeline</figcaption>
                        </figure>
                    </div>

                    <h4>
                        Step 02: Detect abnormal symbols in texts
                    </h4>
                    <p>
                        Symbols such as #, ^, ~, -, and non-Unicode characters negatively affect text quality and do not
                        contribute useful information for language identification.
                    </p>
                    <p>
                        To detect the abnormal symbols, I used <a
                            href="https://docs.python.org/es/3.13/library/unicodedata.html">unicodedata</a> package to
                        detect the character which is not á»‹n types: letter, mark, number
                    </p>

                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/symbols_removal.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Abnormal symbols removing pipeline</figcaption>
                        </figure>
                    </div>

                    <h4>
                        Step 03: Clean the text with high ratio of number
                    </h4>
                    <p>
                        Texts consisting mostly numbers, can be math calculation, equation, or numeric tables, they
                        should also be removed.
                    </p>
                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/digit_process.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Digit processing pipeline</figcaption>
                        </figure>
                    </div>

                    <h4>
                        Step 04: Select mostly character texts
                    </h4>

                    <p>
                        After multiple cleaning steps, some texts may become empty or just contain very little
                        information left, now we need to keep only texts that contain a sufficient proportion of
                        informative characters.
                    </p>
                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/letter_ratio.png" alt="plot1" class="clickable-img" style="width:300px;">
                            <!-- <figcaption></figcaption> -->
                        </figure>
                    </div>

                    <p>
                        The final dataset is stored in parquet files. The storage is significantly reduced compared with
                        the tsv files in the previous step.
                    </p>

                    <h4>
                        Subsampling the data using SemHash and Cleanlab
                    </h4>

                    <p>
                        The data now contains about 48,320,698 samples, which is too large to run the evaluation step on
                        the current device.
                        Therefore, the data must be downsampled, retaining only the highest-quality samples.
                    </p>

                    <p>
                        Here, I used <a href="https://github.com/MinishLab/semhash">SemHash</a>, a lightweight,
                        multimodal library, used for semantic deduplication, outlier filtering, and representative
                        sample selection.
                    </p>

                    <p>
                        SemHash was applied separatedly to each language:
                    </p>
                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/semhash_apply.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>SemHash application pipeline</figcaption>
                        </figure>
                    </div>

                    <p>
                        Finally, the data is stored in many parquet files, one per each language. For more details,
                        visit this <a href="https://www.kaggle.com/code/caokhoihuynh/semhash-apply">notebook</a>
                    </p>

                    <p>
                        In a language dataset, an important issue is language misslabeling.
                        The incorrect labeled samples can significantly affect the training process, and negativly
                        impact the model performance.
                    </p>
                    <p>
                        To solve this problem, I used <a href="https://github.com/cleanlab/cleanlab">Cleanlab</a>, a
                        library which works with any ML model by analyzing model outputs like predicted probabilities
                        and feature embeddings
                        to identify problems such as label errors, outliers, near duplicates, and other data quality
                        issues.

                    </p>
                    <p>
                        In Cleanlab, there are two important elements:
                    </p>

                    <ul>
                        <li>The embeder: Create the embedding vectors of the given text </li>
                        <li>Classifier: Trained from the data, to create the predicted language probabilities for a
                            given text </li>

                    </ul>

                    <p>After trying with multiple setting of embedder and classifier, I have these main settings. The
                        detailed implementation can be found in <a
                            href="https://www.kaggle.com/code/caokhoihuynh/check-the-data-issue-cleanlab">this
                            notebook</a></p>
                    <ul>
                        <li>Embeder: FastText - Classifier: Logistic regression, API:
                            cleanlab.classification.CleanLearning </li>
                        <li>Embeder: Transformer + FastText - Classifier: XGB classifier, API:
                            cleanlab.classification.CleanLearning </li>
                        <li>Embeder: FastText - Classifier: Logistic regression, API: cleanlab.filter.find_label_issues
                        </li>
                        <li>Embeder: Transformer - Classifier: Logistic regression, API:
                            cleanlab.filter.find_label_issues </li>
                    </ul>

                    <p>
                        Different settings will result in a different label-cleaned versions of dataset. From that, I
                        ensembled all of them, using a simple strategy that a text, which will have a highly confident
                        about the label quality, if it appear in all versions.
                        The detailed implementation can be found in <a
                            href="https://www.kaggle.com/code/caokhoihuynh/ensemble-data-02"> this notebook</a>
                    </p>

                    <p>
                        After all of these steps, I have a cleaned version of the dataset, containing about 1,5 million
                        samples. And it still is not completely clean. ðŸ¥²ðŸ¥²ðŸ¥²
                    </p>

                    <h3>
                        Task 03: Doing EDA for the collected dataset
                    </h3>

                    <p>
                        The detailed analysis on the dataset can be found in <a
                            href="https://www.kaggle.com/code/caokhoihuynh/eda-multilingual-dataset">this notebook</a>
                    </p>
                    <p>
                        Some key features about the dataset can be listed:
                    </p>

                    <ul>
                        <li>Number of samples: 1,570,807 </li>
                        <li>Number of supported languages: 1862</li>
                        <li>Number of sources collected: 17</li>
                    </ul>

                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/source_similarity.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Source similarity in Jaccard distance</figcaption>
                        </figure>
                        <figure class="figure-center">
                            <img src="./Images/length_distribution.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Text length distribution</figcaption>
                        </figure>
                        <figure class="figure-center">
                            <img src="./Images/word_count_distribution.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Word count distribution</figcaption>
                        </figure>
                    </div>



                    <h3>
                        Task 04: Evaluate the cleaned dataset with models
                    </h3>
                    <p>
                        Here is the list of model I evaluated, also consider the supporting language list for all these
                        models
                    </p>

                    <ul>
                        <li><a href="https://huggingface.co/papluca/xlm-roberta-base-language-detection">papluca/xlm-roberta-base-language-detection
                                (papluca)<a>: 20 possible labels</li>
                        <li><a href="https://github.com/dleemiller/WordLlamaDetect">WordLlama Detect (llamadetect)</a>
                            146 possible labels</li>
                        <li><a href="https://huggingface.co/julien-c/fasttext-language-id">julien-c/fasttext-language-id
                                (julien_fasttext)</a> 175 possible labels</li>
                        <li><a href="https://huggingface.co/NeuML/language-id">NeuML/language-id (neuml_langid)</a> 175
                            possible labels</li>
                        <li><a href="https://huggingface.co/laurievb/OpenLID-v2">laurievb/OpenLID-v2 (openlid_v2)</a>:
                            194 possible labels</li>
                        <li><a href="https://huggingface.co/alexneakameni/language_detection">alexneakameni/language_detection
                                (alexneakameni)</a>: 195 possible labels</li>
                        <li><a href="https://huggingface.co/facebook/fasttext-language-identification">facebook/fasttext-language-identification
                                (facebook_fasttext)</a> 211 possible labels</li>
                        <li><a href="https://github.com/epfl-nlp/ConLID">epfl-nlp/ConLID (conlid)</a> 1869 possible
                            labels</li>
                        <li><a href="https://huggingface.co/cis-lmu/glotlid">cis-lmu/glotlid (lmu_glotlid)</a>: 1873
                            possible labels</li>
                    </ul>

                    <p>
                        We can see the <strong>papluca</strong> models just support a limited set of languages (20
                        languages), while <strong>conlid</strong> and <strong>glotlid</strong> can support more than
                        1800 languages. Other models can support around 100-200 languages.
                    </p>
                    <p>
                        Here, to ensure a fair evaluation between models, we can consider three scenarios:
                    </p>
                    <ul>
                        <li><strong>Scenario 01</strong> Evaluate all models, except papluca, on a 83 languages dataset
                            (this is the overlap of supported languages of all other models).</li>
                        <li><strong>Scenario 02</strong> Evaluate all models, including papluca, in 17 languages
                            dataset.</li>
                        <li><strong>Scenario 03</strong>: Evaluate only conlid and glotlid in a 1810 languages dataset.
                        </li>
                    </ul>

                    <p>
                        I evaluated each language separately. I stored the accuracy of each model for a language, also
                        the total inference time and the inference time per sample for that language also.
                    </p>

                    <p>
                        Beside that, I also stored the detailed prediction results of all models for each sample, for
                        future analysis.
                    </p>

                    <p>The full evaluatation code can be found in the notebook <a
                            href="https://www.kaggle.com/code/caokhoihuynh/run-evaluation-cpu">CPU evaluation</a> and <a
                            href="https://www.kaggle.com/code/caokhoihuynh/run-evaluation-gpu">GPU evaluation</a></p>

                    <h3>
                        Task 05: Analyze the evaluation results
                    </h3>

                    <p>The detailed analysis can be found in <a
                            href="https://www.kaggle.com/code/caokhoihuynh/evaluate-results-analysis">this notebook</a>
                    </p>
                    <p>Here, I will summarize some key insights:</p>

                    <h4>
                        01. Metrics
                    </h4>

                    <p>I evaluated all these models in two metrics are accuracy and F1-macro score </p>
                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/metric_results.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>RAM usage for CPU evaluation</figcaption>
                        </figure>
                    </div>

                    <p>
                        In the Scenario 01, the evaluation was conducted on around 100 most popular languages.
                        Among evaluated models, <strong>Conlid</strong> and <strong>lmu_glotlid</strong> achive the
                        highest performance in both accuracy and f1 score.
                        These models also have the widest language coverage, which more than 1800 language evaluated in
                        Scenario 03.
                        While the highly performance is not consistenlt maintained in both scenario.
                        One possible reason can be from the evaluating data itself, with many low resouce languages (~
                        700 language just have one sample), it can lead to unfair or unstable evaluating results.
                    </p>

                    <p>
                        In Scenario 02, where the evaluation focused on the top 17 most popular languages, the
                        <strong>papluca</strong> model, which just supports 20 languages, achieved the highest
                        performance.
                        This suggests that the model is primarily trained on popular languages and performs well in
                        simpler, high-resource settings.
                    </p>

                    <p>
                        In addition, the neuml_langid model showed very poor performance in Scenario 01.
                        Although its performance improved in Scenario 02, it still remained significantly lower than the
                        other models.
                    </p>

                    <h4>
                        02. Memory usage
                    </h4>
                    <p>
                        Memory usage is a key factor when we seleted a language identification model.
                        Ideally, we always want to choose a model which is lightweight, but can achive strong
                        performance and fit with practical deployment requirements.
                    </p>

                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/RAM_usage.png" alt="plot1" class="clickable-img" style="width:300px;">
                            <figcaption>RAM usage for CPU evaluation</figcaption>
                        </figure>
                        <figure class="figure-center">
                            <img src="./Images/VRAM_usage.png" alt="plot1" class="clickable-img" style="width:300px;">
                            <figcaption>VRAM usage for GPU evaluation</figcaption>
                        </figure>
                    </div>

                    <p>
                        Look at the RAM usage, we observe that evaluated models requires from hundreds to thousands of
                        megabytes.
                        Only a few transformer-based model can support the GPU execution. Therefore, most evaluations
                        are performed on the CPU.
                        This als reflects a trend in model development nowadays, where models are become more
                        lightweight and optimized for CPU-based inference.
                    </p>

                    <p>
                        In more detail, I used the pareto plot to show the relation between memory usage and the model
                        accuracy.
                        The analysis focuses on Scenario 01 and Scenario 02, where multiple models are compared under
                        the same evaluation settings.
                    </p>



                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/scen_01_ram_vs_metrics.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Pareto plot for RAM usage vs metrics in scenario 01</figcaption>
                        </figure>
                        <figure class="figure-center">
                            <img src="./Images/scen_02_ram_vs_metrics.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Pareto plot for RAM usage vs metrics in scenario 02</figcaption>
                        </figure>
                    </div>


                    <p>
                        From the results, <strong>Conlid</strong> achives the highest accuracy while using an acceptable
                        memory,
                        Additionally, <strong>alexneakameni</strong> is the cheapest model, with competitive accuracy,
                        <strong>llamadectect</strong> also offers a strong balance between low memory consumption and
                        performance
                    </p>


                    <h4>
                        03. Throughtput
                    </h4>

                    <p>
                        The throughput is measured as the number of samples that a model can process per second.
                        This is an important factor for many real-world application that requires a fast and accurate
                        LID module.
                    </p>

                    <div class="figure-container">
                        <figure class="figure-center">
                            <img src="./Images/scen_01_thoughput_vs_metrics.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Pareto plot for RAM usage vs metrics in scenario 01</figcaption>
                        </figure>
                        <figure class="figure-center">
                            <img src="./Images/scen_02_thoughput_vs_metrics.png" alt="plot1" class="clickable-img"
                                style="width:300px;">
                            <figcaption>Pareto plot for RAM usage vs metrics in scenario 02</figcaption>
                        </figure>
                    </div>

                    <p>
                        With the thoughput, <strong>Conlid</strong> model, despite of having the highest accuracy, is
                        the slowest one.
                        This suggests that Conlid will be useful in scenarios where the accuracy is our priority, rather
                        than the memory efficiency or inference speed.
                    </p>


                    <p>
                        The <strong>julien_fasttext</strong> model, which has very low memory consumtion, achives the
                        highest thoughput.
                        However, the overal accuracy of it is not really good.
                        Therefore, it is a suitable option for tasks with strict speed and resouce constrainsts.
                    </p>

                    <p>
                        In addition, the <strong>llamadetect</strong> model offers a balanced trade-off across accuracy,
                        memory usage, and throughput.
                        This makes it a strong candidate for a wide range of practical applications.
                    </p>



                    <h3>Conclusion</h3>
                    <p>Finally, from this project, I have gained many knowledge about the currently language
                        identification methods, and greate experience when working in NLP field</p>

                </div>
            </section>

            <footer class="site-footer">
                <p>&copy; 2025 Cao Khoi Huynh</p>
            </footer>
        </main>

    </div>
    <script src="../../../js/sidebar.js"></script>
    <script src="../../../js/post-navigation.js"></script>
    <script src="../../../js/modal.js"></script>

</body>

</html>